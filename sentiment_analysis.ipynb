{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "#nltk.download('stopwords') # first time installation for stopwords from nltk, comment out if done priorly\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText\n",
       "0       1          0                       is so sad for my APL frie...\n",
       "1       2          0                     I missed the New Moon trail...\n",
       "2       3          1                            omg its already 7:30 :O\n",
       "3       4          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4       5          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read from file\n",
    "twitter_df = pd.read_csv('train.csv', encoding = 'latin-1')\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                         is so sad for my APL frie...\n",
      "1                       I missed the New Moon trail...\n",
      "2                              omg its already 7:30 :O\n",
      "3              .. Omgaga. Im sooo  im gunna CRy. I'...\n",
      "4             i think mi bf is cheating on me!!!   ...\n",
      "5                    or i just worry too much?        \n",
      "6                   Juuuuuuuuuuuuuuuuussssst Chillin!!\n",
      "7           Sunny Again        Work Tomorrow  :-|  ...\n",
      "8          handed in my uniform today . i miss you ...\n",
      "9             hmmmm.... i wonder how she my number @-)\n",
      "Name: SentimentText, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(twitter_df.SentimentText.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText  \\\n",
       "0       1          0                       is so sad for my APL frie...   \n",
       "1       2          0                     I missed the New Moon trail...   \n",
       "2       3          1                            omg its already 7:30 :O   \n",
       "3       4          0            .. Omgaga. Im sooo  im gunna CRy. I'...   \n",
       "4       5          0           i think mi bf is cheating on me!!!   ...   \n",
       "\n",
       "   Length  \n",
       "0      61  \n",
       "1      51  \n",
       "2      37  \n",
       "3     132  \n",
       "4      53  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a new row/attribute of length\n",
    "twitter_df['Length'] = twitter_df['SentimentText'].apply(len)\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    99989.000000\n",
      "mean        77.264309\n",
      "std         36.382639\n",
      "min          6.000000\n",
      "25%         47.000000\n",
      "50%         73.000000\n",
      "75%        108.000000\n",
      "max        949.000000\n",
      "Name: Length, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x2925eddf780>,\n",
       "  <matplotlib.lines.Line2D at 0x2925eddfac8>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x2925eddfe10>,\n",
       "  <matplotlib.lines.Line2D at 0x2925eddfd68>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x2925eddf358>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x2925edf54e0>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x2925edf5828>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(twitter_df['Length'].describe())\n",
    "plt.boxplot(twitter_df.Length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x2925ee6ef28>,\n",
       "  <matplotlib.lines.Line2D at 0x2925ee3c2b0>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x2925ee3c5f8>,\n",
       "  <matplotlib.lines.Line2D at 0x2925ee3c940>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x2925ee6eb38>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x2925ee3cc88>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x2925ee3cfd0>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAD3NJREFUeJzt3V+InfWdx/H3dycxkXSzJjiKOHEjy7CMDawtgwjOhSGwNV4Ye9HVuWhCZyC9MENLe2OdCw2LWoK12LAVUhK00B030NYKmt0VDXQHtn/GIjbubGlobJ1mMJM1tDplYjJ+92Ke6ETPZP6cOTlzfr5f8HDO+Z3nz3cC+ZyH3/N7fk9kJpKkcv1VswuQJDWWQS9JhTPoJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuEMekkq3KpmFwBw9dVX5+bNm5tdhiS1lFdeeeV0ZrbPt96KCPrNmzczMjLS7DIkqaVExO8Xsp5dN5JUOINekgpn0EtS4Qx6SSqcQS9JhTPopTkMDQ2xZcsW2tra2LJlC0NDQ80uSVqSFTG8UlpphoaGGBwc5ODBg/T09DA8PEx/fz8Avb29Ta5OWpxYCY8S7O7uTsfRayXZsmUL+/fvZ+vWrR+0HT16lIGBAY4dO9bEyqQPRcQrmdk973oGvfRxbW1tTE1NsXr16g/azp07x9q1a5menm5iZdKHFhr09tFLNXR1dTE8PHxR2/DwMF1dXU2qSFo6g16qYXBwkP7+fo4ePcq5c+c4evQo/f39DA4ONrs0adG8GCvVcOGC68DAAKOjo3R1dfHwww97IVYtyT56SWpR9tFLkgCDXpKKZ9BLUuEMekkqnEEvSYUz6CWpcAa9NAdnr1QpvGFKqsHZK1USb5iSanD2SrWCZbthKiI2RcTRiBiNiNcj4itV+0MR8ceIeLVa7py1zTci4nhE/CYiPlffnyJdfqOjo/T09FzU1tPTw+joaJMqkpZuIX3054GvZ2YXcCtwX0TcVH337cy8uVpeAKi+uxf4NHAH8N2IaGtA7VLDOHulSjJv0GfmeGb+qnr/DjAKXH+JTXYAz2Tm2cw8ARwHblmOYqXLxdkrVZJFXYyNiM3AZ4CfA7cBeyJiJzDCzFn/GWZ+BH42a7MxLv3DIK04zl6pkiw46CPiU8APga9m5p8j4kngn4GsXr8F9AFRY/OPXfGNiN3AboAbbrhh8ZVLDdbb22uwqwgLGkcfEauZCfkfZOaPADLzrcyczsz3ge/xYffMGLBp1uYdwMmP7jMzD2Rmd2Z2t7e31/M3SA3hOHqVYiGjbgI4CIxm5uOz2q+btdrngQtjzp4D7o2INRFxI9AJ/GL5SpYa78I4+v379zM1NcX+/fsZHBw07NWS5h1HHxE9wH8Bvwber5ofAHqBm5nplnkD+HJmjlfbDDLTjXOema6eI5c6huPotdI4jl6tYKHj6L1hSqqhra2NqakpVq9e/UHbuXPnWLt2LdPT002sTPqQT5iS6uA4epXEoJdqcBy9SmLQSzX09vbS2dnJtm3buOKKK9i2bRudnZ0Ot1RLMuilGgYGBnj55Zd57LHHmJyc5LHHHuPll19mYGCg2aVJi+bFWKmGtWvX8sgjj/C1r33tg7bHH3+cBx54gKmpqSZWJn3Ii7FSHc6ePcvGjRsvumFq48aNnD17ttmlSYtm0Es1rFq1ioGBASYnJ8lMJicnGRgYYNUqn9Wj1mPQSzWsWbOGd999l+3bt3PmzBm2b9/Ou+++y5o1a5pdmrRoBr1Uw+TkJHfddReHDh3iqquu4tChQ9x1111MTk42uzRp0Qx6aQ579uxhamqKzGRqaoo9e/Y0uyRpSQx6qYaOjg527dp10Q1Tu3btoqOjo9mlSYtm0Es17Nu3j/Pnz9PX18fatWvp6+vj/Pnz7Nu3r9mlSYtm0Es19Pb2cs899zA+Ps7777/P+Pg499xzj3fGqiUZ9FINQ0NDPP/88xw5coT33nuPI0eO8PzzzzsfvVqSd8ZKNTgfvVqB89FLdXA+erUCp0CQ6uB89CqJQS/V4Hz0KokTd0g1XBhdMzAwwOjoKF1dXTz88MOOulFLso9eklqUffRSnYaGhi6aptihlWpVdt1INQwNDTE4OMjBgwfp6elheHiY/v5+ALtv1HLsupFqcBy9WoHj6KU6OI5ercA+eqkOXV1d7N2796I++r179zqOXi3JoJdq2Lp1K48++iinT58mMzl9+jSPPvroRV05Uqsw6KUann32WdavX8+VV15JRHDllVeyfv16nn322WaXJi2aQS/VMDY2xuHDhzlx4gTT09OcOHGCw4cPMzY21uzSpEWbN+gjYlNEHI2I0Yh4PSK+UrVvjIgXI+K31euGqj0i4jsRcTwiXouIzzb6j5AkzW0hZ/Tnga9nZhdwK3BfRNwE3A+8lJmdwEvVZ4DtQGe17AaeXPaqpQbr6Ohg586dF811s3PnTh8lqJY0b9Bn5nhm/qp6/w4wClwP7ACerlZ7Gri7er8D+H7O+BlwVURct+yVSw20b98+pqen6evrY82aNfT19TE9Pe2jBNWSFtVHHxGbgc8APweuzcxxmPkxAK6pVrseeHPWZmNVm9Qyent7eeKJJ1i3bh0Rwbp163jiiSe8K1YtacFTIETEp4AfAl/NzD9HxJyr1mj72F1ZEbGbma4dbrjhhoWWIV02vb29BruKsKAz+ohYzUzI/yAzf1Q1v3WhS6Z6PVW1jwGbZm3eAZz86D4z80Bmdmdmd3t7+1LrlxYlIi7LIq0kCxl1E8BBYDQzH5/11XPArur9LuAns9p3VqNvbgX+dKGLR2q2zFz0spTtpJVkIV03twFfBH4dEa9WbQ8A3wQOR0Q/8AfgC9V3LwB3AseBvwBfWtaKJUmLMm/QZ+YwtfvdAbbVWD+B++qsS5K0TLwzVpIKZ9BLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgpn0EtS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuEMekkqnEEvSYWbN+gj4lBEnIqIY7PaHoqIP0bEq9Vy56zvvhERxyPiNxHxuUYVLklamIWc0T8F3FGj/duZeXO1vAAQETcB9wKfrrb5bkS0LVexkqTFmzfoM/OnwNsL3N8O4JnMPJuZJ4DjwC111CdJqlM9ffR7IuK1qmtnQ9V2PfDmrHXGqjZJUpMsNeifBP4OuBkYB75VtUeNdbPWDiJid0SMRMTIxMTEEsuQJM1nSUGfmW9l5nRmvg98jw+7Z8aATbNW7QBOzrGPA5nZnZnd7e3tSylDkrQASwr6iLhu1sfPAxdG5DwH3BsRayLiRqAT+EV9JUqS6rFqvhUiYgi4Hbg6IsaAB4HbI+JmZrpl3gC+DJCZr0fEYeB/gPPAfZk53ZjSJUkLEZk1u9Avq+7u7hwZGWl2GVJNEcFK+H8ifVREvJKZ3fOt552xklQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgpn0EtS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klQ4g16SCreq2QVIS7Vx40bOnDlzWY4VEQ3d/4YNG3j77bcbegx9chn0allnzpwhM5tdxrJo9A+JPtnm7bqJiEMRcSoijs1q2xgRL0bEb6vXDVV7RMR3IuJ4RLwWEZ9tZPGSpPktpI/+KeCOj7TdD7yUmZ3AS9VngO1AZ7XsBp5cnjIlSUs1b9Bn5k+Bj3Ye7gCert4/Ddw9q/37OeNnwFURcd1yFStJWryljrq5NjPHAarXa6r264E3Z603VrVJkppkuYdX1rqiVPNqWUTsjoiRiBiZmJhY5jIkSRcsNejfutAlU72eqtrHgE2z1usATtbaQWYeyMzuzOxub29fYhmSpPksNeifA3ZV73cBP5nVvrMafXMr8KcLXTySpOaYdxx9RAwBtwNXR8QY8CDwTeBwRPQDfwC+UK3+AnAncBz4C/ClBtQsSVqEeYM+M3vn+GpbjXUTuK/eoiRJy8e5biSpcAa9JBXOoJekwhn0klQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgpn0EtS4eadplhaqfLB9fDQ3zS7jGWRD65vdgkqmEGvlhV7/8zMIxBaX0SQDzW7CpXKrhtJKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgpn0EtS4Qx6SSqcQS9JhTPoJalwBr0kFa6uSc0i4g3gHWAaOJ+Z3RGxEfg3YDPwBvBPmXmmvjIlSUu1HGf0WzPz5szsrj7fD7yUmZ3AS9VnSVKTNKLrZgfwdPX+aeDuBhxDkrRA9QZ9Av8ZEa9ExO6q7drMHAeoXq+ptWFE7I6IkYgYmZiYqLMMSdJc6n3wyG2ZeTIirgFejIj/XeiGmXkAOADQ3d1dxtMjJGkFquuMPjNPVq+ngB8DtwBvRcR1ANXrqXqLlCQt3ZKDPiLWRcRfX3gP/CNwDHgO2FWttgv4Sb1FSnOJiCKWDRs2NPufUgWrp+vmWuDHEXFhP/+amf8eEb8EDkdEP/AH4Av1lyl93OV6XmxEFPNsWn0yLTnoM/N3wD/UaP8/YFs9RUmSlo93xkpS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgpn0EtS4Qx6SSqcQS9JhVvV7AKkyykiLst2mbmk40iN0LAz+oi4IyJ+ExHHI+L+Rh1HWozMvCyLtJI0JOgjog34F2A7cBPQGxE3NeJYkqRLa9QZ/S3A8cz8XWa+BzwD7GjQsSRJl9CooL8eeHPW57Gq7QMRsTsiRiJiZGJiokFlSJIaFfS1rlxd1HGZmQcyszszu9vb2xtUhiSpUUE/Bmya9bkDONmgY0mSLqFRQf9LoDMiboyIK4B7gecadCxJ0iU0ZBx9Zp6PiD3AfwBtwKHMfL0Rx5IkXVrDbpjKzBeAFxq1f0nSwsRKuLkjIiaA3ze7DmkOVwOnm12EVMPfZua8o1lWRNBLK1lEjGRmd7PrkJbKSc0kqXAGvSQVzqCX5neg2QVI9bCPXpIK5xm9JBXOoJfmEBGHIuJURBxrdi1SPQx6aW5PAXc0uwipXga9NIfM/CnwdrPrkOpl0EtS4Qx6SSqcQS9JhTPoJalwBr00h4gYAv4b+PuIGIuI/mbXJC2Fd8ZKUuE8o5ekwhn0klQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQV7v8BEOxg5uZDCPcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Drop any invalid tweet with length more that 280\n",
    "invalidTweet = twitter_df[ twitter_df.Length > 280].index # get the index of invalid tweet with length more than 280\n",
    "twitter_df = twitter_df.drop(invalidTweet)\n",
    "twitter_df.reset_index(drop=True,inplace=True) # Reset index\n",
    "plt.boxplot(twitter_df.Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data cleaning function\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fats\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:273: UserWarning: \"b' i just received my G8 viola exam.. and its... well... .. disappointing.. :\\\\..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 processed\n",
      "10000 processed\n",
      "10000 processed\n",
      "10000 processed\n",
      "10000 processed\n",
      "10000 processed\n",
      "10000 processed\n",
      "10000 processed\n",
      "10000 processed\n",
      "Process completed\n"
     ]
    }
   ],
   "source": [
    "clean_tweet_texts = []\n",
    "for i in range(0,len(twitter_df)):\n",
    "    if( (i+1) % 10000 == 0):\n",
    "        print(\"10000 processed\")\n",
    "    clean_tweet_texts.append(tweet_cleaner(twitter_df['SentimentText'][i]))\n",
    "print(\"Process completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data into csv\n",
    "clean_df = pd.DataFrame(clean_tweet_texts, columns=['text'])\n",
    "clean_df['target'] = twitter_df.Sentiment\n",
    "clean_df.to_csv('clean_tweet.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is so sad for my apl friend</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i missed the new moon trailer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg its already o</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>omgaga im sooo im gunna cry i ve been at this ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i think mi bf is cheating on me t t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0                        is so sad for my apl friend       0\n",
       "1                      i missed the new moon trailer       0\n",
       "2                                  omg its already o       1\n",
       "3  omgaga im sooo im gunna cry i ve been at this ...       0\n",
       "4                i think mi bf is cheating on me t t       0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanTwitter_df = pd.read_csv('clean_tweet.csv',index_col=0)\n",
    "cleanTwitter_df.dropna(inplace=True)\n",
    "cleanTwitter_df.reset_index(drop=True,inplace=True) # Reset index\n",
    "cleanTwitter_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def translator(text):\n",
    "    text = text.split(\" \")\n",
    "    j = 0\n",
    "    for word in text:\n",
    "        with open(\"translate.txt\",\"r\") as translateFile:\n",
    "            # reading file as CSV with delimter \"=\", abbreviation stored in row[0], phrase in row[1]\n",
    "            dataFromFile = csv.reader(translateFile, delimiter= \"=\")\n",
    "            for row in dataFromFile:\n",
    "                # check if selected word matches short forms[LHS] in text file\n",
    "                if word.upper() == row[0]:\n",
    "                    # if match, replace with phrase[RHS]\n",
    "                    text[j] = row[1].lower()\n",
    "            translateFile.close();\n",
    "        j = j + 1\n",
    "\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nopunc.split() will be like ['Sample', 'message', 'is' , 'hi']\n",
    "def text_process(mess):\n",
    "    # Check characters to see if they are in punctuation\n",
    "    # if char is not in string.punctuation(a library) then nopunc = that char\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "\n",
    "    #Convert slangs or short form\n",
    "    noshortform = translator(nopunc) # call the translator function\n",
    "        \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in noshortform.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                          is so sad for my apl friend\n",
      "1                        i missed the new moon trailer\n",
      "2                                    omg its already o\n",
      "3    omgaga im sooo im gunna cry i ve been at this ...\n",
      "4                  i think mi bf is cheating on me t t\n",
      "5                             or i just worry too much\n",
      "6                     juuuuuuuuuuuuuuuuussssst chillin\n",
      "7                 sunny again work tomorrow tv tonight\n",
      "8        handed in my uniform today i miss you already\n",
      "9                     hmmmm i wonder how she my number\n",
      "Name: text, dtype: object\n",
      "\n",
      "\n",
      "---------------------compare---------------\n",
      "\n",
      "\n",
      "0                                   [sad, apl, friend]\n",
      "1                         [missed, new, moon, trailer]\n",
      "2                                   [oh, god, already]\n",
      "3    [omgaga, sooo, gunna, cry, dentist, since, sup...\n",
      "4                                [think, bf, cheating]\n",
      "5                                        [worry, much]\n",
      "6                  [juuuuuuuuuuuuuuuuussssst, chillin]\n",
      "7                 [sunny, work, tomorrow, tv, tonight]\n",
      "8              [handed, uniform, today, miss, already]\n",
      "9                              [hmmmm, wonder, number]\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# For visualization, the first 5 is now tokenized with punctuation and stopwords removed\n",
    "from nltk.corpus import stopwords\n",
    "print(cleanTwitter_df['text'].head(10))\n",
    "print(\"\\n\\n---------------------compare---------------\\n\\n\")\n",
    "print(cleanTwitter_df['text'].head(10).apply(text_process))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#bow = bag of words\n",
    "bow_transformer = CountVectorizer(analyzer=text_process).fit(cleanTwitter_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i missed the new moon trailer\n",
      "\n",
      "0                                   [sad, apl, friend]\n",
      "1                         [missed, new, moon, trailer]\n",
      "2                                   [oh, god, already]\n",
      "3    [omgaga, sooo, gunna, cry, dentist, since, sup...\n",
      "4                                [think, bf, cheating]\n",
      "Name: text, dtype: object\n",
      "  (0, 29320)\t1\n",
      "  (0, 29770)\t1\n",
      "  (0, 31070)\t1\n",
      "  (0, 46570)\t1\n"
     ]
    }
   ],
   "source": [
    "# for visualiztion of count vector\n",
    "message2 = cleanTwitter_df['text'][1]\n",
    "print(message2 + \"\\n\")\n",
    "print(cleanTwitter_df['text'].head(5).apply(text_process))\n",
    "\n",
    "bow4 = bow_transformer.transform([message2])\n",
    "print(bow4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_bow = bow_transformer.transform(cleanTwitter_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer().fit(text_bow)\n",
    "# transform entire bag of words into TF-IDF\n",
    "text_tfidf = tfidf_transformer.transform(text_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "sentiment_detect_model = MultinomialNB().fit(text_tfidf,cleanTwitter_df['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "all_predictions = sentiment_detect_model.predict(text_tfidf)\n",
    "print(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.70      0.77     43364\n",
      "           1       0.80      0.91      0.85     56247\n",
      "\n",
      "   micro avg       0.82      0.82      0.82     99611\n",
      "   macro avg       0.83      0.81      0.81     99611\n",
      "weighted avg       0.83      0.82      0.82     99611\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(cleanTwitter_df['target'], all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Text_train, Text_test , Sentiment_train, Sentiment_test = \\\n",
    "train_test_split(cleanTwitter_df['text'], cleanTwitter_df['target'] , test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('bow', CountVectorizer(analyzer=<function text_process at 0x0000029263C21BF8>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), preprocesso...f=False, use_idf=True)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(Text_train,Sentiment_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(Text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test split validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.75      0.65      6549\n",
      "           1       0.85      0.72      0.78     13374\n",
      "\n",
      "   micro avg       0.73      0.73      0.73     19923\n",
      "   macro avg       0.71      0.74      0.71     19923\n",
      "weighted avg       0.76      0.73      0.74     19923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( classification_report(predictions,Sentiment_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.77      0.66      3191\n",
      "           1       0.87      0.73      0.79      6771\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      9962\n",
      "   macro avg       0.72      0.75      0.72      9962\n",
      "weighted avg       0.77      0.74      0.75      9962\n",
      "\n",
      "--------------------------------\n",
      "Iteration  2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.76      0.65      3304\n",
      "           1       0.85      0.71      0.78      6657\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      9961\n",
      "   macro avg       0.71      0.73      0.71      9961\n",
      "weighted avg       0.76      0.73      0.73      9961\n",
      "\n",
      "--------------------------------\n",
      "Iteration  3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.76      0.65      3202\n",
      "           1       0.87      0.72      0.79      6759\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      9961\n",
      "   macro avg       0.72      0.74      0.72      9961\n",
      "weighted avg       0.77      0.74      0.74      9961\n",
      "\n",
      "--------------------------------\n",
      "Iteration  4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.76      0.66      3309\n",
      "           1       0.86      0.72      0.79      6652\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      9961\n",
      "   macro avg       0.72      0.74      0.72      9961\n",
      "weighted avg       0.77      0.74      0.74      9961\n",
      "\n",
      "--------------------------------\n",
      "Iteration  5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.76      0.66      3282\n",
      "           1       0.86      0.73      0.79      6679\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      9961\n",
      "   macro avg       0.72      0.74      0.72      9961\n",
      "weighted avg       0.77      0.74      0.74      9961\n",
      "\n",
      "--------------------------------\n",
      "Iteration  6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.75      0.64      3259\n",
      "           1       0.85      0.71      0.78      6702\n",
      "\n",
      "   micro avg       0.72      0.72      0.72      9961\n",
      "   macro avg       0.71      0.73      0.71      9961\n",
      "weighted avg       0.76      0.72      0.73      9961\n",
      "\n",
      "--------------------------------\n",
      "Iteration  7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.76      0.66      3292\n",
      "           1       0.86      0.73      0.79      6669\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      9961\n",
      "   macro avg       0.72      0.74      0.72      9961\n",
      "weighted avg       0.77      0.74      0.75      9961\n",
      "\n",
      "--------------------------------\n",
      "Iteration  8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.77      0.66      3280\n",
      "           1       0.86      0.72      0.79      6681\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      9961\n",
      "   macro avg       0.72      0.74      0.72      9961\n",
      "weighted avg       0.77      0.74      0.74      9961\n",
      "\n",
      "--------------------------------\n",
      "Iteration  9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.66      3312\n",
      "           1       0.86      0.73      0.79      6649\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      9961\n",
      "   macro avg       0.72      0.74      0.72      9961\n",
      "weighted avg       0.76      0.74      0.74      9961\n",
      "\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits = 10, shuffle = True)\n",
    "\n",
    "# X = Text (message \"Hi how are you\")\n",
    "# Y = target sentiment 0,1\n",
    "\n",
    "iteration = 1\n",
    "for train_index ,test_index in kf.split(cleanTwitter_df):\n",
    "    X_train, X_test, Y_train, Y_test = cleanTwitter_df.text[train_index], cleanTwitter_df.text[test_index], \\\n",
    "    cleanTwitter_df.target[train_index],cleanTwitter_df.target[test_index]\n",
    "    pipeline.fit(X_train,Y_train) # use training set \n",
    "    \n",
    "    predictions = pipeline.predict(X_test)\n",
    "    print(\"Iteration \", iteration )\n",
    "    print(classification_report(predictions,Y_test))\n",
    "    print(\"--------------------------------\" )\n",
    "    \n",
    "    iteration = iteration +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
